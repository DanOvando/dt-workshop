---
title: "Eco-data-science data.table workshop"
output: html_notebook

---

### shortcuts

Run Chunk : *Cmd+Shift+Enter*

Insert Chunk : *Cmd+Option+I*

Preview HTML : *Cmd+Shift+K*

# Introduction

## About the data

The datasets we will be using in this interactive session are taken from the Environmental Protection Agency's (EPA) Air Markets Program Data (AMPD) program: https://ampd.epa.gov/ampd/. 

The first dataset (the ``epa_ampd_hourly_2019_selected.csv`` file) is a dataset of hourly emissions, electriticity generation, and fuel consumption for coal and natural gas across the country. Because the full dataset downloaded from the AMPD site is extremely large, we have filtered out the dataset to only have generators in the states of Alaska, California, Minnesota, New Jersey, New York, and Texas. The following columns are included in the dataset:

| Column name                 | Description                           |
| :---                        |    :---                               |   
| STATE                       | state where facility (power plant) is located       |
| FACILITY_NAME               | name of facility (power plant)        |
| ORISPL_CODE                 | the plant ID code assigned by the Department of Energy's Energy Information Administration (EIA)       |
| UNITID                      | identifier for generating units (generator) at the facility (power plant)        |
| OP_DATE                     | the date on which a particular pollutant concentration or flow rate was recorded        |
| OP_HOUR                     | the clock hour of the day in which a particular pollutant concentration or flow rate was recorded (range of values 00-23)        |
| OP_TIME                     | the fraction of the clock hour during which the unit combusted any fuel        |
| GLOAD (MW)                  | amount of electricity generated during that hour in megawatts (MW)        |
| SLOAD (1000lb/hr)           | steam load in units of 1,000 lbs (per hour)        |
| SO2_MASS (lbs)              | mass of SO2 emitted in units of lbs        |
| SO2_MASS_MEASURE_FLG        | code number indicating if reported SO2 mass was measured or substituted        |
| SO2_RATE (lbs/mmBtu)        | rate of SO2 emitted per fuel consumed in units of lbs per mmBtu        |
| NOX_RATE (lbs/mmBtu)        | rate of NOx emitted per fuel consumed in units of lbs per mmBtu         |
| NOX_MASS (lbs)              | mass of NOx emitted in units of lbs        |
| NOX_MASS_MEASURE_FLG        | code number indicating if reported NOx mass was measured or substituted        |
| CO2_MASS (tons)             | mass of CO2 emitted in units of metric tons        |
| CO2_MASS_MEASURE_FLG        | code number indicating if reported CO2 mass was measured or substituted        |
| CO2_RATE (tons/mmBtu)       | rate of CO2 emitted per fuel consumed in units of lbs per mmBtu        |
| CO2_RATE_MEASURE_FLG        | code number indicating if reported CO2 rate was measured or substituted        |
| HEAT_INPUT (mmBtu)          | the calculated heat input rate for the hour in million Btu (mmBtu)        |
| FAC_ID                      | EPA-specific facility ID        |
| UNIT_ID                     | EPA-specific unit id        |


The second dataset (the ``facility_01-27-2021_224024745.csv`` file) is a list of generators and various attributes, such as their fuel type, generator type, and location. The columns included in the dataset are:

| Column name                 | Description                           |
| :---                        | :---                                  |   
| State                       | state where facility (power plant) is located       |
| Facility Name               | name of facility (power plant)        |
| Facility ID (ORISPL)        | the plant ID code assigned by the Department of Energy's Energy Information Administration (EIA)       |
| Unit ID                     | identifier for generating units (generator) at the facility (power plant)        |
| Year                        | year of facility data       |
| Facility Latitude           | latitude of facility        |
| Facility Longitude          | longitude of facility       |
| Unit Type                   | type of generating unit        |
| Fuel Type (Primary)         | primary fuel burned at generating unit        |
| Fuel Type (Secondary)       | secondary fuel (if there is one) burned at generating unit        |
| Operating Status            | status on whether generating unit is operating during year of data        |


# Packages

## Install packages

Along with the ``data.table`` package, we will also be using the ``here`` package for this notebook. The ``here`` package basically helps us [write something here]. 
We also briefly use the ``janitor`` package to clean up column names.
```{r}
# install.packages("data.table")
# install.packages("here")
# install.packages("janitor")
```

## Load packages

Load the ``data.table`` and ``janitor`` packages. We don't need to load the ``here`` package because we can explicitly call on the package when using the here function (``here::here()``).
```{r}
library(data.table)
library(janitor)
```

# Read in data

## File names
Here, we are just explicitly setting objects/variables for the data files, so we can easily use them later.
```{r}
hourly_data     = 'epa_ampd_hourly_2019_selected.csv'
facility_data   = 'facility_01-27-2021_224024745.csv'
```

## Read in CSV files using ``fread``

Use the ``fread`` function to read in the hourly data. 
```{r}
dt = fread(here::here("data", hourly_data))
```

View first few rows of the hourly data:
```{r}
head(dt)
```
We can see that the column names are not ideal (spaces and parantheses are not easy to work with). Thus, let's use the ``clean_names`` package in the ``janitor`` package to clean up the column names:
```{r}
dt = clean_names(dt)
head(dt)
```
Nice! Now the column names are all lower case, the spaces have been replaced with underscores, and the parantheses and slashes are gone. There are some problematic column names (for example, mmBtu has been changed to mm_btu, but we will fix those columns later).

# ``set`` functions

## ``setkey()``

Set keys to enable fast repeated lookup in specified columns using ``dt[.(value), ]``, particularly for columns/keys that you frequently use (e.g., if you keep summarizing across the same groups). Setting keys can also be used for merging without specifying merging columns using ``dt_a[dt_b]``.
We're going to set the keys of ``dt`` as the **orispl_code**, **unitid**, **op_date**, and **op_hour** columns:
```{r}
setkey(dt, orispl_code, unitid, op_date, op_hour)
```

## ``setnames()``

### Rename a single column
```{r}
setnames(dt, "heat_input_mm_btu", "heat_input_mmbtu")
head(dt)
```

### Rename multiple columns
```{r}
setnames(dt, c("so2_rate_lbs_mm_btu", "nox_rate_lbs_mm_btu", "co2_rate_tons_mm_btu"), c("so2_rate_lbs_mmbtu", "nox_rate_lbs_mmbtu", "co2_rate_tons_mmbtu"))
head(dt)
```
## ``setorder()``

### Temporarily sorting
```{r}
dt[order(-gload_mw)]
```

### Actually sorting the data table in place

Order ``dt`` by the orispl_code column:
```{r}
setorder(dt, orispl_code, na.last = TRUE)
head(dt)
```

We can also order the datable by multiple columns. For example, let's say we want ``dt`` to be ordered by the following columns (in this order): state, orispl_code, unitid, op_date, and op_hour. 
```{r}
setorder(dt, state, orispl_code, unitid, op_date, op_hour)
head(dt)
```
## ``setcolorder()``

The ``setcolorder()`` function can be used to set the order of columns of a data table. So, let's reorder the columns to be in the following order:
state, facility_name, orispl_code, unitid, op_date, op_hour, op_time, gload_mw, heat_input_mmbtu, co2_mass_tons, so2_mass_lbs, nox_mass_lbs, sload_1000lb_hr, co2_rate_tons_mmbtu, so2_rate_lbs_mmbtu, nox_rate_lbs_mmbtu, co2_mass_measure_flg, co2_rate_measure_flg, so2_mass_measure_flg, so2_rate_measure_flg, nox_mass_measure_flg, nox_rate_measure_flg, fac_id, unit_id.

```{r}
setcolorder(dt, c("state", "facility_name", "orispl_code", "unitid", "op_date", "op_hour", "op_time", "gload_mw", "heat_input_mmbtu", "co2_mass_tons", "so2_mass_lbs", "nox_mass_lbs", "sload_1000lb_hr", "co2_rate_tons_mmbtu", "so2_rate_lbs_mmbtu", "nox_rate_lbs_mmbtu", "co2_mass_measure_flg", "co2_rate_measure_flg", "so2_mass_measure_flg", "so2_rate_measure_flg", "nox_mass_measure_flg", "nox_rate_measure_flg", "fac_id", "unit_id"))
head(dt)
```

# Filter/subset rows using i

In this section, we will go over how to filter ``dt``'s rows in a few different ways.

## Filter using row numbers
One way to subset rows is by specifying row numbers. For example, let's say we want just the first three rows of ``dt``:
```{r}
dt[1:3,]
```

## Filter using operators

Some of the operators that can be used to filter/subset rows are: <, >, >=, <=, is.na(), !is.na(), is.infinite(), !is.infinite(), %in%, %like%, %between%, %chin%, and probably others we're not thinking of right now. When filtering, ``!`` represents **not** matching, ``|`` represents **or**, and ``&`` represents **and**. 

Let's filter ``dt`` for all rows where the **state** is California.
```{r}
dt[state=="CA"]
```
What about if we want to filter for rows where the state is California or New Jersey?
```{r}
dt[state %chin% c("CA", "NJ")]
```
A less efficient way would be to use the ``|`` operator:
```{r}
dt[state == "CA" | state == "NJ"]
```

The operators can also be used to filter dates. Let's filter for all rows that occurred after July 1, 2019:
```{r}
dt[op_date > "07-01-2019"]
```
The ``%like`` operator can be used to filter for rows that match a substring. For example, we can filter for rows where the facility name has the word "Energy" in it:
```{r}
dt[facility_name %like% "Energy"]
```

All of the filters thus far only print the resulting rows but do not actually save to a data.table. Let's filter out our existing ``dt`` data table to only have rows where neither the ``gload_mw`` or the ``heat_input_mm_btu`` columns are NA.
```{r}
dt = dt[!is.na(gload_mw) & !is.na(heat_input_mmbtu)]
head(dt)
```

# Select and manipulate columns using j

## Select columns by column number/index
```{r}
dt[, c(3:8)]
```

## Select columns by name
```{r}
dt[, .(orispl_code, unitid, op_date, op_hour, op_time, gload_mw)]
```

Let's only keep the following columns (note that this will also reorder the columns):

* state
* orispl_code
* unitid
* op_date
* op_hour
* gload_mw
* so2_mass_lbs
* nox_mass_lbs
* co2_mass_tons
* heat_input_mm_btu
```{r}
dt = dt[, .(state, orispl_code, unitid, op_date, op_hour, gload_mw, heat_input_mmbtu, co2_mass_tons, so2_mass_lbs, nox_mass_lbs)]
head(dt)
```
## Add columns

### Add simple columns
```{r}
dt[, a := 1]
head(dt)
```

### Add column based on subset of rows
```{r}
dt[op_date <= "06-01-2019", b := 2]
head(dt)
```

### Add column based on cases (using ``fifelse`` and ``fcase``)

We can add columns based on whether or not (yes or no) a condition is met (and the condition can be based on other columns). To do this, we can use the ``fifelse`` function (which is a faster version of ``ifelse``). The syntax of ``fifelse`` is ``fifelse(test, yes, no, na=NA)``. The ``fifelse`` is comparable to ``dyplr``'s ``if_else`` function.

Let's say we want to create a new column called ``c`` that is based on whether or not the date is before or after June 1, 2019. If before, the ``c`` column value is "before june"; if not, the ``c`` value is "june onwards.
```{r}
dt[, c := fifelse(op_date <= "06-01-2019", "before june", "june onwards")]
head(dt)
```

```{r}
tail(dt)
```

Another useful function to know is ``fcase``, which is like a nested version of ``fifelse``. So, let's say we have multiple conditions we want to use to create columns based on. Instead of doing something like ``fifelse(test_1, yes, fifelse(test_2, yes, fifelse(test_3, yes, no)))``, we can just use ``fcase``. ``fcase`` is comparable to dplyr’s ``case_when()``. The basic syntax of ``fcase`` is ``fcase(condition1, "value1", condition2, "value2", etc)`` 
```{r}
dt[, d := fcase(gload_mw < 100, "small",
                gload_mw %between% c(100, 300), "small-ish",
                gload_mw %between% c(300, 500), "medium",
                gload_mw > 500, "large")]
```

```{r}
head(dt[gload_mw == 500])
```

```{r}
head(dt[gload_mw > 500])
```

### Add column using other columns

Let's extract the month as the column ``op_month``:
```{r}
dt[, op_month := substr(op_date, 1, 2)]
head(dt)
```

### Convert column type
To change an existing column, you can just use the same method without changing the column name. For example, we can see that currently, the ``gload_mw`` is of the integer type.
```{r}
str(dt)
```

When really it should be a numeric value. So let's change the column to a numeric type:
```{r}
dt[, gload_mw := as.numeric(gload_mw)]
str(dt)
```

## Add multiple columns

One way to add multiple columns is to use the LHS := RHS method
```{r}
dt[, c("heat_rate_mmbtu_mwh", "efficiency") := .(gload_mw/heat_input_mmbtu, (heat_input_mmbtu/gload_mw)*3.412)]
head(dt)
```
Another way to add multiple columns is to use the functional form.
```{r}
dt[, ':=' (co2_rate_tons_mwh = co2_mass_tons/gload_mw, nox_rate_lbs_mwh = nox_mass_lbs/gload_mw, so2_rate_lbs_mwh = so2_mass_lbs/gload_mw)]
head(dt)
```


## Delete columns

A column can be deleted using ``dt[, col := NULL]``. For example, let's deleted the column ``a`` we created earlier:
```{r}
dt[, a := NULL]
head(dt)
```

We can also delete multiple columns at once. Let's delete the columns b, c, and d as well.
```{r}
dt[, c("b", "c", "d") := NULL]
head(dt)
```
Similar to how we added new columns using the functional form, we can also remove columns using the functional form. Let's delete the ``co2_rate_tons_mwh``, ``nox_rate_lbs_mwh``, and ``so2_rate_lbs_mwh`` columns we created earlier.
```{r}
dt[, ':=' (co2_rate_tons_mwh = NULL, nox_rate_lbs_mwh = NULL, so2_rate_lbs_mwh = NULL)]
head(dt)
```

# Group according to **by**

## Summarize by group(s)

Let's find the total amount of CO2 emitted by state and by date:
```{r}
dt[, .(total_co2_tons = sum(co2_mass_tons, na.rm = T)), by = .(op_date)]
```

## Summarize several columns

``.SD`` can be used to summarize multiple columns
```{r}
dt[, lapply(.SD, sum, na.rm = T), .SDcols = c("co2_mass_tons", "so2_mass_lbs", "nox_mass_lbs")]
```

## Summarize several columns by group(s)

We can also use ``.SD`` by group:
```{r}
dt[, lapply(.SD, sum, na.rm = T), .SDcols = c("co2_mass_tons", "so2_mass_lbs", "nox_mass_lbs"), by = .(state)]
```
## Summarize multiple functions on several columns by group(s)

The ``.N`` special character can be used to count the number of observations. Not to be confused with the ``uniqueN`` function, which counts the number of *unique* observations. Let's find the number of observations, the number of power plants, and the total generation by state:
```{r}
dt[, .(number_of_observations = .N,
       number_of_plants = uniqueN(orispl_code),
       total_generation = sum(gload_mw, na.rm = T)), by = .(state)]
```

## Add columns by groups

Let's say we want to add a column that's the max generation for each generator:
```{r}
dt[, max_gen := max(gload_mw, na.rm = T), by = .(orispl_code, unitid)]
head(dt)
```

## Adding multiple columns applying a function based on groups
Let's say I want to create a new column that's the mean of the NOx, SO2, and SO2 columns for each generator within each month. 
```{r}
cols = c("co2_mass_tons", "so2_mass_lbs", "nox_mass_lbs")
dt[, paste0(cols, "_mean") := lapply(.SD, mean), .SDcols = cols, by = .(orispl_code, unitid, op_month)] 
head(dt)
```

## Add columns based on sequential rows

Within groups, we can compute a column with sequential row IDs.
```{r}
dt[, no_rows := 1:.N, by = .(orispl_code, unitid)]
head(dt)
```
What if we want to compute a new column that calculates the difference in CO2 emissions from the previous hour for every generator?
```{r}
dt[, diff_co2 := co2_mass_tons - shift(co2_mass_tons, type = "lag"), by = .(orispl_code, unitid)]
tail(dt)
```

# Reshape between long and wide forms

## Reshape from wide to long form using ``melt``

```{r}
dt_long = melt(dt, 
               id.vars = c("orispl_code", "unitid", "op_date", "op_hour"), 
               measure.vars = c("co2_mass_tons", "so2_mass_lbs", "nox_mass_lbs"))
head(dt_long)
```

## Reshape from long to wide form using ``dcast``

```{r}
dt_wide = dcast(dt_long, orispl_code + unitid + op_date + op_hour ~ variable)
head(dt_wide)
```

# Joining data

## Read in facility data
```{r}
fac_dt = fread(here::here('data', facility_data))
head(fac_dt)
```
Fix column names with the ``janitor`` package:
```{r}
fac_dt = clean_names(fac_dt)
head(fac_dt)
```

## Merge

### Merge using the ``dt_a[dt_b]`` form
```{r}
combined_dt = dt[fac_dt, on = .(state = state, orispl_code = facility_id_orispl, unitid = unit_id), nomatch = 0]
head(combined_dt)
```

### Merge using the ``merge()`` function
Merge (version 1):
```{r}
combined_v2 = merge(dt, fac_dt, 
      by.x = c("state", "orispl_code", "unitid"),
      by.y = c("state", "facility_id_orispl", "unit_id"))
head(combined_v2)
```

## Bind

### Bind rows using ``rbind()``

### Bind columns using ``cbind()``


# Beyond...

Can we calculate which 10 power plants (not generators) emitted the most CO2?

How much electricity generation, NOx emissions, SO2 emissions, etc are coming from coal vs natural gas plants?